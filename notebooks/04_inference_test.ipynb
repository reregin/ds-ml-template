{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80a8ea7e",
   "metadata": {},
   "source": [
    "# Inference Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9690a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "from src import config\n",
    "from src.inference import make_prediction, load_model\n",
    "\n",
    "print(\"‚úÖ Libraries loaded. Ready to test inference.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aa1e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the clean data just to get a sample row\n",
    "df = pd.read_csv(f\"../{config.PROCESSED_DATA_PATH}\")\n",
    "\n",
    "# Pick a random row (e.g., row #100)\n",
    "sample_row = df.iloc[100]\n",
    "print(\"--- Sample Input Data ---\")\n",
    "print(sample_row)\n",
    "\n",
    "# Separate Target (We don't send the answer to the model!)\n",
    "ground_truth = sample_row[config.TARGET_COLUMN]\n",
    "input_data = sample_row.drop(config.TARGET_COLUMN).to_dict()\n",
    "\n",
    "print(\"\\n--- Input Payload (JSON-like) ---\")\n",
    "print(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1112d9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚è≥ Calling make_prediction()...\")\n",
    "\n",
    "# START TIMER\n",
    "start_time = time.time()\n",
    "\n",
    "# CALL THE FUNCTION\n",
    "result = make_prediction(input_data)\n",
    "\n",
    "# STOP TIMER\n",
    "end_time = time.time()\n",
    "latency = (end_time - start_time) * 1000 # Convert to ms\n",
    "\n",
    "print(f\"\\n‚úÖ Prediction Result: {result}\")\n",
    "print(f\"‚è±Ô∏è Latency: {latency:.2f} ms\")\n",
    "print(f\"üéØ Actual Value (Ground Truth): {ground_truth}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffbd1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"running batch stress test...\")\n",
    "success_count = 0\n",
    "errors = []\n",
    "\n",
    "# Take 100 random samples\n",
    "batch = df.sample(100).drop(columns=[config.TARGET_COLUMN]).to_dict(orient='records')\n",
    "\n",
    "for i, record in enumerate(batch):\n",
    "    try:\n",
    "        res = make_prediction(record)\n",
    "        if res['status'] == 'Success':\n",
    "            success_count += 1\n",
    "    except Exception as e:\n",
    "        errors.append(e)\n",
    "\n",
    "print(f\"Batch Test Complete: {success_count}/100 successful.\")\n",
    "if errors:\n",
    "    print(f\"First Error: {errors[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

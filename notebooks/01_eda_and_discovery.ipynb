{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f3168a0",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49363d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import skew, kurtosis\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Dynamic Path Setup\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "from src import config\n",
    "\n",
    "# Visual Settings\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929e6aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_PATH = '../data/raw/dataset.csv' # CHANGE THIS\n",
    "df_raw = pd.read_csv(RAW_DATA_PATH)\n",
    "\n",
    "print(f\"Raw data shape: {df_raw.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04df38da",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8b08f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COL = 'target' # Define the target variable here\n",
    "\n",
    "# Defaulting to a standard 80/20 split. Adjust stratify for classification tasks.\n",
    "df_train, df_test = train_test_split(\n",
    "    df_raw, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=df_raw[TARGET_COL] if df_raw[TARGET_COL].dtype == 'object' else None\n",
    ")\n",
    "\n",
    "print(f\"Training data shape: {df_train.shape}\")\n",
    "print(f\"Test data shape: {df_test.shape} (Set aside until final evaluation)\")\n",
    "\n",
    "# For the rest of the notebook, we strictly use df_train\n",
    "df = df_train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6e4ac4",
   "metadata": {},
   "source": [
    "### \"Bird's Eye\" View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e06a543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peek at the data\n",
    "display(df.head())\n",
    "\n",
    "# Data types and missing values\n",
    "info_df = pd.DataFrame({\n",
    "    'Data Type': df.dtypes,\n",
    "    'Missing Values': df.isnull().sum(),\n",
    "    'Missing %': (df.isnull().sum() / len(df)) * 100,\n",
    "    'Unique Values': df.nunique()\n",
    "})\n",
    "display(info_df.sort_values(by='Missing %', ascending=False))\n",
    "\n",
    "# Baseline statistical summary\n",
    "display(df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f23c93",
   "metadata": {},
   "source": [
    "### Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2aa9532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features by type for programmatic plotting\n",
    "num_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "if TARGET_COL in num_cols: num_cols.remove(TARGET_COL)\n",
    "if TARGET_COL in cat_cols: cat_cols.remove(TARGET_COL)\n",
    "\n",
    "# Target Distribution\n",
    "plt.figure(figsize=(8, 4))\n",
    "if df[TARGET_COL].dtype in ['int64', 'float64']:\n",
    "    sns.histplot(df[TARGET_COL], kde=True)\n",
    "else:\n",
    "    sns.countplot(x=df[TARGET_COL])\n",
    "plt.title(f\"Target Variable Distribution: {TARGET_COL}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb2115b",
   "metadata": {},
   "source": [
    "### Numeric Feature Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40be7f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "if num_cols:\n",
    "    cols_per_row = 4\n",
    "    n_rows = (len(num_cols) + cols_per_row - 1) // cols_per_row\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, cols_per_row, figsize=(18, n_rows * 4))\n",
    "    axes = axes.flatten() # Safe because cols_per_row > 1\n",
    "    \n",
    "    for idx, col in enumerate(num_cols):\n",
    "        axes[idx].boxplot(df[col].dropna(), vert=True, patch_artist=True, boxprops=dict(facecolor='lightblue'))\n",
    "        axes[idx].set_title(f'{col}', fontsize=12, fontweight='bold')\n",
    "        axes[idx].set_ylabel('Value')\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for idx in range(len(num_cols), len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffde9e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skewness and Kurtosis for numeric features\n",
    "if num_cols:\n",
    "    dist_stats = []\n",
    "    for col in num_cols:\n",
    "        dist_stats.append({\n",
    "            'Feature': col,\n",
    "            'Skewness': round(skew(df[col].dropna()), 3),\n",
    "            'Kurtosis': round(kurtosis(df[col].dropna()), 3)\n",
    "        })\n",
    "    \n",
    "    dist_df = pd.DataFrame(dist_stats)\n",
    "    display(dist_df.sort_values(by='Skewness', key=abs, ascending=False))\n",
    "    \n",
    "    # Histogram + KDE plots for numeric features\n",
    "    cols_per_row = 4\n",
    "    n_rows = (len(num_cols) + cols_per_row - 1) // cols_per_row\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, cols_per_row, figsize=(18, n_rows * 4))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, col in enumerate(num_cols):\n",
    "        sns.histplot(df[col].dropna(), kde=True, ax=axes[idx], color='skyblue')\n",
    "        axes[idx].set_title(f'{col}', fontsize=12, fontweight='bold')\n",
    "        axes[idx].set_ylabel('Frequency')\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for idx in range(len(num_cols), len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No numeric columns for distribution analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767bb377",
   "metadata": {},
   "source": [
    "### Categorical Feature Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d4b613",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cat_cols:\n",
    "    cols_per_row = 3\n",
    "    n_rows = (len(cat_cols) + cols_per_row - 1) // cols_per_row\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, cols_per_row, figsize=(18, n_rows * 4))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    MAX_CATEGORIES = 15 # Safety valve for high-cardinality columns\n",
    "    \n",
    "    for idx, col in enumerate(cat_cols):\n",
    "        value_counts = df[col].value_counts()\n",
    "        \n",
    "        # Apply safety valve\n",
    "        if len(value_counts) > MAX_CATEGORIES:\n",
    "            top_counts = value_counts.iloc[:MAX_CATEGORIES]\n",
    "            other_count = pd.Series({'...Others': value_counts.iloc[MAX_CATEGORIES:].sum()})\n",
    "            value_counts = pd.concat([top_counts, other_count])\n",
    "            \n",
    "        axes[idx].bar(range(len(value_counts)), value_counts.values, color='steelblue')\n",
    "        axes[idx].set_xticks(range(len(value_counts)))\n",
    "        axes[idx].set_xticklabels(value_counts.index, rotation=45, ha='right')\n",
    "        axes[idx].set_title(f'{col}', fontsize=12, fontweight='bold')\n",
    "        axes[idx].set_ylabel('Count')\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for idx in range(len(cat_cols), len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0f4311",
   "metadata": {},
   "source": [
    "### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501daa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric Correlation Matrix\n",
    "if len(num_cols) > 1:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    corr_matrix = df[num_cols + ([TARGET_COL] if df[TARGET_COL].dtype in ['int64', 'float64'] else [])].corr()\n",
    "    \n",
    "    # Mask upper triangle\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    sns.heatmap(corr_matrix, mask=mask, annot=False, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    plt.title(\"Numeric Feature Correlation Matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87508b48",
   "metadata": {},
   "source": [
    "### Missing Data Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08ce8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value heatmap\n",
    "missing_data = df.isnull()\n",
    "\n",
    "if missing_data.any().any():\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.heatmap(missing_data, cbar=True, cmap='viridis', yticklabels=False)\n",
    "    plt.title(\"Missing Data Heatmap (Yellow = Missing)\")\n",
    "    plt.xlabel(\"Features\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Missing data correlation\n",
    "    if missing_data.sum().sum() > 0:\n",
    "        missing_corr = missing_data.corr()\n",
    "        \n",
    "        # Only show if there's meaningful correlation between missing values\n",
    "        if (missing_corr.abs() > 0.5).sum().sum() > len(missing_corr):\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(missing_corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
    "            plt.title(\"Missing Data Correlation (Do missing values occur together?)\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "else:\n",
    "    print(\"‚úì No missing data detected in the training set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b29ea44",
   "metadata": {},
   "source": [
    "### Target Relationship Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30864c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric Features vs Target\n",
    "if num_cols and df[TARGET_COL].dtype in ['int64', 'float64']:\n",
    "    # For regression: scatter plots\n",
    "    cols_per_row = 3\n",
    "    n_rows = (len(num_cols) + cols_per_row - 1) // cols_per_row\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, cols_per_row, figsize=(18, n_rows * 4))\n",
    "    if n_rows == 1 and cols_per_row == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for idx, col in enumerate(num_cols):\n",
    "        axes[idx].scatter(df[col], df[TARGET_COL], alpha=0.5)\n",
    "        axes[idx].set_xlabel(col)\n",
    "        axes[idx].set_ylabel(TARGET_COL)\n",
    "        axes[idx].set_title(f'{col} vs {TARGET_COL}')\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    for idx in range(len(num_cols), len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "elif num_cols and df[TARGET_COL].dtype == 'object':\n",
    "    # For classification: box plots by target class\n",
    "    cols_per_row = 3\n",
    "    n_rows = (len(num_cols) + cols_per_row - 1) // cols_per_row\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, cols_per_row, figsize=(18, n_rows * 4))\n",
    "    if n_rows == 1 and cols_per_row == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for idx, col in enumerate(num_cols):\n",
    "        df.boxplot(column=col, by=TARGET_COL, ax=axes[idx])\n",
    "        axes[idx].set_title(f'{col} by {TARGET_COL}')\n",
    "        axes[idx].set_xlabel(TARGET_COL)\n",
    "        axes[idx].set_ylabel(col)\n",
    "    \n",
    "    for idx in range(len(num_cols), len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f38819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical Features vs Target\n",
    "if cat_cols:\n",
    "    cols_per_row = 2\n",
    "    n_rows = (len(cat_cols) + cols_per_row - 1) // cols_per_row\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, cols_per_row, figsize=(16, n_rows * 5))\n",
    "    if n_rows == 1 and cols_per_row == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for idx, col in enumerate(cat_cols):\n",
    "        # Cross-tabulation of categorical feature with target\n",
    "        ct = pd.crosstab(df[col], df[TARGET_COL], normalize='index') * 100\n",
    "        ct.plot(kind='bar', ax=axes[idx], stacked=False)\n",
    "        axes[idx].set_title(f'{col} vs {TARGET_COL}')\n",
    "        axes[idx].set_xlabel(col)\n",
    "        axes[idx].set_ylabel('Percentage' if df[TARGET_COL].dtype == 'object' else 'Count')\n",
    "        axes[idx].legend(title=TARGET_COL)\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    for idx in range(len(cat_cols), len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505a533c",
   "metadata": {},
   "source": [
    "### Outlier Detection (IQR Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dc95cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify outliers using IQR method\n",
    "if num_cols:\n",
    "    outlier_summary = []\n",
    "    \n",
    "    for col in num_cols:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "        outlier_count = len(outliers)\n",
    "        outlier_pct = (outlier_count / len(df)) * 100\n",
    "        \n",
    "        outlier_summary.append({\n",
    "            'Feature': col,\n",
    "            'Outlier Count': outlier_count,\n",
    "            'Outlier %': round(outlier_pct, 2),\n",
    "            'Lower Bound': round(lower_bound, 2),\n",
    "            'Upper Bound': round(upper_bound, 2)\n",
    "        })\n",
    "    \n",
    "    outlier_df = pd.DataFrame(outlier_summary).sort_values(by='Outlier %', ascending=False)\n",
    "    display(outlier_df)\n",
    "    \n",
    "    # Visualize outlier percentage\n",
    "    if outlier_df['Outlier %'].sum() > 0:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.barh(outlier_df['Feature'], outlier_df['Outlier %'], color='coral')\n",
    "        plt.xlabel('Outlier Percentage (%)')\n",
    "        plt.title('Outlier Detection by Feature (IQR Method)')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No numeric columns to check for outliers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6665531e",
   "metadata": {},
   "source": [
    "### Temporal Analysis (If Applicable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faba78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect and analyze time-based features\n",
    "date_cols = []\n",
    "for col in df.columns:\n",
    "    # Try to detect date columns by attempting to convert to datetime\n",
    "    if df[col].dtype == 'object':\n",
    "        try:\n",
    "            pd.to_datetime(df[col], errors='raise')\n",
    "            date_cols.append(col)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "if date_cols:\n",
    "    print(f\"Detected potential date columns: {date_cols}\")\n",
    "    \n",
    "    for col in date_cols:\n",
    "        df[f'{col}_datetime'] = pd.to_datetime(df[col])\n",
    "        \n",
    "        # Time series plot of target over time\n",
    "        plt.figure(figsize=(14, 5))\n",
    "        \n",
    "        if df[TARGET_COL].dtype in ['int64', 'float64']:\n",
    "            # For regression: line plot of target over time\n",
    "            df_time = df.sort_values(f'{col}_datetime')\n",
    "            plt.plot(df_time[f'{col}_datetime'], df_time[TARGET_COL], alpha=0.7)\n",
    "            plt.title(f'{TARGET_COL} over Time ({col})')\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel(TARGET_COL)\n",
    "        else:\n",
    "            # For classification: stacked area chart of target classes over time\n",
    "            df_time = df.groupby([pd.Grouper(key=f'{col}_datetime', freq='M'), TARGET_COL]).size().unstack(fill_value=0)\n",
    "            df_time.plot(kind='area', stacked=True, alpha=0.7)\n",
    "            plt.title(f'{TARGET_COL} Distribution over Time ({col})')\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel('Count')\n",
    "        \n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No temporal features detected. If you have date columns, convert them manually.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b2bbec",
   "metadata": {},
   "source": [
    "### Cardinality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0c51ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify high-cardinality categorical features\n",
    "if cat_cols:\n",
    "    cardinality_info = []\n",
    "    \n",
    "    for col in cat_cols:\n",
    "        unique_count = df[col].nunique()\n",
    "        cardinality_pct = (unique_count / len(df)) * 100\n",
    "        \n",
    "        cardinality_info.append({\n",
    "            'Feature': col,\n",
    "            'Unique Values': unique_count,\n",
    "            'Cardinality %': round(cardinality_pct, 2),\n",
    "            'Type': 'High' if unique_count > 50 else ('Medium' if unique_count > 10 else 'Low')\n",
    "        })\n",
    "    \n",
    "    cardinality_df = pd.DataFrame(cardinality_info).sort_values(by='Unique Values', ascending=False)\n",
    "    display(cardinality_df)\n",
    "    \n",
    "    # Flag high-cardinality features that may need special encoding\n",
    "    high_card_features = cardinality_df[cardinality_df['Type'] == 'High']['Feature'].tolist()\n",
    "    if high_card_features:\n",
    "        print(f\"\\n‚ö†Ô∏è High-cardinality features detected: {high_card_features}\")\n",
    "        print(\"   Consider: Target encoding, frequency encoding, or feature hashing.\")\n",
    "else:\n",
    "    print(\"No categorical columns to check for cardinality.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f635d78",
   "metadata": {},
   "source": [
    "### Class Imbalance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa97317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for class imbalance (for classification tasks)\n",
    "if df[TARGET_COL].dtype == 'object' or df[TARGET_COL].nunique() < 20:\n",
    "    value_counts = df[TARGET_COL].value_counts()\n",
    "    value_pcts = df[TARGET_COL].value_counts(normalize=True) * 100\n",
    "    \n",
    "    imbalance_df = pd.DataFrame({\n",
    "        'Class': value_counts.index,\n",
    "        'Count': value_counts.values,\n",
    "        'Percentage': [f\"{pct:.2f}%\" for pct in value_pcts.values]\n",
    "    })\n",
    "    \n",
    "    print(\"Class Distribution:\")\n",
    "    display(imbalance_df)\n",
    "    \n",
    "    # Calculate imbalance ratio\n",
    "    if len(value_counts) == 2:\n",
    "        imbalance_ratio = value_counts.max() / value_counts.min()\n",
    "        print(f\"\\nImbalance Ratio: {imbalance_ratio:.2f}:1\")\n",
    "        \n",
    "        if imbalance_ratio > 3:\n",
    "            print(\"‚ö†Ô∏è Significant class imbalance detected!\")\n",
    "            print(\"   Consider: SMOTE, class weights, or stratified sampling.\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Count plot\n",
    "    axes[0].bar(range(len(value_counts)), value_counts.values, color='steelblue')\n",
    "    axes[0].set_xticks(range(len(value_counts)))\n",
    "    axes[0].set_xticklabels(value_counts.index, rotation=45)\n",
    "    axes[0].set_title('Class Distribution (Count)')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Pie chart\n",
    "    axes[1].pie(value_counts.values, labels=value_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "    axes[1].set_title('Class Distribution (Percentage)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Target variable is continuous (regression task). Class imbalance analysis not applicable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5ac3f6",
   "metadata": {},
   "source": [
    "### Multicollinearity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab53223c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIF (Variance Inflation Factor) Analysis\n",
    "if len(num_cols) > 1:\n",
    "    # Prepare data for VIF (drop NaNs)\n",
    "    vif_data = df[num_cols].dropna()\n",
    "    \n",
    "    if len(vif_data) > 0 and vif_data.shape[1] > 1:\n",
    "        vif_values = []\n",
    "        \n",
    "        for i, col in enumerate(vif_data.columns):\n",
    "            try:\n",
    "                vif = variance_inflation_factor(vif_data.values, i)\n",
    "                vif_values.append({\n",
    "                    'Feature': col,\n",
    "                    'VIF': round(vif, 2)\n",
    "                })\n",
    "            except:\n",
    "                vif_values.append({\n",
    "                    'Feature': col,\n",
    "                    'VIF': 'Error'\n",
    "                })\n",
    "        \n",
    "        vif_df = pd.DataFrame(vif_values).sort_values(by='VIF', ascending=False, key=lambda x: pd.to_numeric(x, errors='coerce'))\n",
    "        display(vif_df)\n",
    "        \n",
    "        print(\"\\nüí° VIF Interpretation:\")\n",
    "        print(\"   VIF < 5: Low multicollinearity\")\n",
    "        print(\"   VIF 5-10: Moderate multicollinearity\")\n",
    "        print(\"   VIF > 10: High multicollinearity (consider removing)\")\n",
    "    \n",
    "    # High correlation pairs\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"High Correlation Pairs (|r| > 0.8):\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    corr_matrix = df[num_cols].corr()\n",
    "    high_corr_pairs = []\n",
    "    \n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            corr_val = corr_matrix.iloc[i, j]\n",
    "            if abs(corr_val) > 0.8:\n",
    "                high_corr_pairs.append({\n",
    "                    'Feature 1': corr_matrix.columns[i],\n",
    "                    'Feature 2': corr_matrix.columns[j],\n",
    "                    'Correlation': round(corr_val, 3)\n",
    "                })\n",
    "    \n",
    "    if high_corr_pairs:\n",
    "        high_corr_df = pd.DataFrame(high_corr_pairs).sort_values(by='Correlation', key=abs, ascending=False)\n",
    "        display(high_corr_df)\n",
    "        print(\"\\n‚ö†Ô∏è Consider removing one feature from each highly correlated pair.\")\n",
    "    else:\n",
    "        print(\"‚úì No highly correlated feature pairs detected (|r| > 0.8).\")\n",
    "else:\n",
    "    print(\"Not enough numeric columns for multicollinearity analysis.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

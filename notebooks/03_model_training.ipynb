{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b7bbb92",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1f61e5",
   "metadata": {},
   "source": [
    "### Setup MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a85661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import os\n",
    "\n",
    "# Set tracking URI (relative to notebook location)\n",
    "mlflow.set_tracking_uri(\"sqlite:///../mlflow.db\")\n",
    "\n",
    "# Set experiment name (update to your project name)\n",
    "mlflow.set_experiment(\"YourProjectName\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1e9d09",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b2128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import joblib\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    roc_auc_score, confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# Add your model imports here\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "# from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eeb5e5e",
   "metadata": {},
   "source": [
    "### Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e645e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = '../data/processed/train_fe.csv'\n",
    "TEST_DATA_PATH = '../data/processed/test_fe.csv'\n",
    "TARGET_COL = 'target'  # Update to your target column name\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv(TRAIN_DATA_PATH)\n",
    "test_df = pd.read_csv(TEST_DATA_PATH)\n",
    "\n",
    "# Split features and target\n",
    "X_train = train_df.drop(columns=[TARGET_COL])\n",
    "y_train = train_df[TARGET_COL]\n",
    "\n",
    "X_test = test_df.drop(columns=[TARGET_COL])\n",
    "y_test = test_df[TARGET_COL]\n",
    "\n",
    "print(f\"Train shapes: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Test shapes:  {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba5ec81",
   "metadata": {},
   "source": [
    "### Load Preprocessing Pipeline (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9f9aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessing artifacts if needed for inference\n",
    "# PREPROCESSING_PATH = '../models/preprocessing.joblib'\n",
    "# preprocessing_pipeline = joblib.load(PREPROCESSING_PATH)\n",
    "# print(\"Preprocessing pipeline loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e071c3",
   "metadata": {},
   "source": [
    "### Define Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f155445c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Update with your model-specific parameters\n",
    "model_params = {\n",
    "    \"param1\": \"value1\",\n",
    "    \"param2\": \"value2\",\n",
    "    \"random_state\": 42\n",
    "}\n",
    "\n",
    "# Data processing parameters for tracking\n",
    "data_params = {\n",
    "    \"data_split_ratio\": \"80/20\",\n",
    "    \"preprocessing_steps\": \"Describe your preprocessing steps\",\n",
    "    \"feature_engineering\": \"Describe your FE approach\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988d6ddc",
   "metadata": {},
   "source": [
    "### Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32b9dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"EXP_01_BaselineModel\"):\n",
    "    \n",
    "    # --- Log Parameters ---\n",
    "    mlflow.log_params(model_params)\n",
    "    mlflow.log_params(data_params)\n",
    "    \n",
    "    mlflow.log_param(\"input_rows\", X_train.shape[0])\n",
    "    mlflow.log_param(\"input_cols\", X_train.shape[1])\n",
    "    mlflow.log_param(\"column_names\", X_train.columns.tolist())\n",
    "    \n",
    "    # Optional: Log notebooks as artifacts\n",
    "    # mlflow.log_artifact(\"01_eda.ipynb\", artifact_path=\"code_snapshot\")\n",
    "    # mlflow.log_artifact(\"02_preprocessing.ipynb\", artifact_path=\"code_snapshot\")\n",
    "    \n",
    "    # --- Train Model ---\n",
    "    print(\"Training model...\")\n",
    "    # model = YourModel(**model_params)\n",
    "    # model.fit(X_train, y_train)\n",
    "    \n",
    "    # --- Make Predictions ---\n",
    "    # y_pred = model.predict(X_test)\n",
    "    # y_prob = model.predict_proba(X_test)[:, 1]  # For binary classification\n",
    "    \n",
    "    # --- Log Metrics ---\n",
    "    # metrics = {\n",
    "    #     \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "    #     \"precision\": precision_score(y_test, y_pred, average='weighted', zero_division=0),\n",
    "    #     \"recall\": recall_score(y_test, y_pred, average='weighted', zero_division=0),\n",
    "    #     \"f1_score\": f1_score(y_test, y_pred, average='weighted', zero_division=0),\n",
    "    #     \"roc_auc\": roc_auc_score(y_test, y_prob)  # For binary classification\n",
    "    # }\n",
    "    # mlflow.log_metrics(metrics)\n",
    "    # print(f\"Logged Metrics: {metrics}\")\n",
    "    \n",
    "    # --- Log Model ---\n",
    "    # mlflow.sklearn.log_model(model, \"model\")\n",
    "    \n",
    "    # --- Log Confusion Matrix ---\n",
    "    # cm = confusion_matrix(y_test, y_pred)\n",
    "    # fig = plt.figure(figsize=(8, 6))\n",
    "    # sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    # plt.ylabel('Actual')\n",
    "    # plt.xlabel('Predicted')\n",
    "    # plt.title('Confusion Matrix')\n",
    "    # mlflow.log_figure(fig, \"confusion_matrix.png\")\n",
    "    # plt.close(fig)\n",
    "    \n",
    "    # --- Feature Importance (if applicable) ---\n",
    "    # if hasattr(model, 'feature_importances_'):\n",
    "    #     importance_df = pd.DataFrame({\n",
    "    #         'feature': X_train.columns,\n",
    "    #         'importance': model.feature_importances_\n",
    "    #     }).sort_values('importance', ascending=False)\n",
    "    #     \n",
    "    #     fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    #     top_n = 20\n",
    "    #     plot_data = importance_df.head(top_n).sort_values('importance', ascending=True)\n",
    "    #     ax.barh(plot_data['feature'], plot_data['importance'])\n",
    "    #     ax.set_xlabel('Importance')\n",
    "    #     ax.set_title(f'Top {top_n} Feature Importances')\n",
    "    #     mlflow.log_figure(fig, 'feature_importance.png')\n",
    "    #     plt.close(fig)\n",
    "    #     \n",
    "    #     # Save importance data\n",
    "    #     os.makedirs('../importance', exist_ok=True)\n",
    "    #     importance_path = '../importance/feature_importance.csv'\n",
    "    #     importance_df.to_csv(importance_path, index=False)\n",
    "    #     mlflow.log_artifact(importance_path, artifact_path='feature_importance')\n",
    "    \n",
    "    print(\"=== Run Complete ===\")\n",
    "    print(\"Check MLflow UI for results: mlflow ui --backend-store-uri sqlite:///../mlflow.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b1c014",
   "metadata": {},
   "source": [
    "### Model Comparison (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e4b44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare multiple models in separate runs\n",
    "# models = {\n",
    "#     \"RandomForest\": RandomForestClassifier(**rf_params),\n",
    "#     \"XGBoost\": XGBClassifier(**xgb_params),\n",
    "#     \"LightGBM\": LGBMClassifier(**lgbm_params)\n",
    "# }\n",
    "# \n",
    "# for model_name, model in models.items():\n",
    "#     with mlflow.start_run(run_name=f\"EXP_01_{model_name}\"):\n",
    "#         # Training and logging code here\n",
    "#         pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

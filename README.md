# ğŸ›¡ï¸ Data Science Project Template

![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)
![Streamlit](https://img.shields.io/badge/Streamlit-App-ff4b4b.svg)
![MLflow](https://img.shields.io/badge/MLflow-Tracking-0194E2.svg)
![Status](https://img.shields.io/badge/Status-Development-green.svg)
![License](https://img.shields.io/badge/License-MIT-lightgrey.svg)

A production-ready, modular structure for Data Science and Machine Learning projects. Designed to separate **experimentation** (notebooks) from **engineering** (src), ensuring reproducibility and scalability from Day 1.

**Key Features:**
- ğŸ”¬ **MLflow Integration** - Automated experiment tracking and model versioning
- ğŸ›¡ï¸ **Data Leakage Prevention** - Split-first strategy ensuring test data never contaminates training
- ğŸ“¦ **Modular Design** - Clean separation between research and production code

---

## ğŸ“‚ Project Structure

This project follows a strict separation of concerns.

```text
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                  # Immutable original data (do not edit)
â”‚   â”œâ”€â”€ processed/            # Cleaned data used for modeling
â”‚   â””â”€â”€ external/             # Third-party data/references
â”‚
â”œâ”€â”€ notebooks/                              # Experimental Laboratory
â”‚   â”œâ”€â”€ 01_eda_and_discovery.ipynb          # Discovery & Analysis (Split-First: Train Only)
â”‚   â”œâ”€â”€ 02_preprocessing.ipynb              # Data Cleaning & Transformation
â”‚   â”œâ”€â”€ 03_model_training.ipynb             # Model Training with MLflow Tracking
â”‚   â””â”€â”€ 04_inference_test.ipynb             # Final Pipeline Validation
â”‚
â”œâ”€â”€ src/                      # Production Codebase
â”‚   â”œâ”€â”€ config.py             # Global Control Center (Paths, Params)
â”‚   â”œâ”€â”€ data_loader.py        # Robust Data Ingestion
â”‚   â”œâ”€â”€ preprocessing.py      # Reusable Cleaning Logic
â”‚   â”œâ”€â”€ train.py              # Model Training Pipeline
â”‚   â””â”€â”€ inference.py          # Prediction Engine (Singleton)
â”‚
â”œâ”€â”€ models/                   # Serialized Models (.pkl, .pth)
â”œâ”€â”€ app/
â”‚   â””â”€â”€ main.py               # User Interface (Streamlit/FastAPI)
â”œâ”€â”€ mlflow.db                 # MLflow Experiment Tracking Database
â””â”€â”€ requirements.txt          # Dependencies
```

---

## ğŸ”¬ MLflow Experiment Tracking

This template integrates **MLflow** for comprehensive experiment tracking and model management.

### Setup
```python
import mlflow

# Set tracking URI (SQLite database)
mlflow.set_tracking_uri("sqlite:///mlflow.db")

# Set experiment name
mlflow.set_experiment("YourProjectName")
```

### What Gets Tracked
- **Parameters**: Model hyperparameters, data split ratios, preprocessing steps
- **Metrics**: Accuracy, precision, recall, F1, RMSE, RÂ², etc.
- **Artifacts**: Model files, plots, confusion matrices
- **Metadata**: Input shape, feature names, timestamps

### View Experiments
```bash
mlflow ui --backend-store-uri sqlite:///mlflow.db
```
Access the dashboard at `http://localhost:5000`

---

## ğŸ›¡ï¸ Data Leakage Prevention Strategy

This template implements a **split-first approach** to eliminate data leakage risks:

### The Problem
Traditional workflows often perform EDA and preprocessing on the full dataset before splitting, which can lead to:
- Target leakage from feature engineering
- Scaling/imputation contaminated by test data statistics
- Overly optimistic model performance

### Our Solution: Split First, Always
1. **Load raw data** â†’ Immediately split into train/test (80/20)
2. **EDA on train only** â†’ All analysis, visualizations, and statistical summaries use training data exclusively
3. **Preprocessing fitted on train** â†’ Transformers learn only from training data
4. **Test set remains untouched** â†’ Locked away until final evaluation

### Implementation
```python
# âœ… Correct: Split FIRST
df = pd.read_csv('data.csv')
train, test = train_test_split(df, test_size=0.2, random_state=42)

# Now use 'train' for all EDA and preprocessing
# 'test' is set aside until model evaluation
```

**Result**: Your model's test performance accurately reflects real-world generalization.

---

## ğŸ“¦ Dependencies

Core libraries used in this template:

```
pandas
numpy
scikit-learn
mlflow
joblib
matplotlib
seaborn
streamlit
```

Install all dependencies:
```bash
pip install -r requirements.txt